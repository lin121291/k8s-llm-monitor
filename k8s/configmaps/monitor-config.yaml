apiVersion: v1
kind: ConfigMap
metadata:
  name: monitor-config
  namespace: ai-monitor
data:
  monitor.yaml: |
    llm:
      model: "llama-3.2-3b"
      max_tokens: 2048
      temperature: 0.1
      inference_server: "vllm"
    
    scaling:
      prediction_window: "30m"
      scale_threshold: 0.8
      min_replicas: 2
      max_replicas: 20
      cooldown_period: "5m"
    
    monitoring:
      prometheus_url: "http://prometheus:9090"
      grafana_url: "http://grafana:3000"
      log_retention: "7d"
    
    kafka:
      bootstrap_servers: "kafka:9092"
      topics:
        logs: "service-logs"
        metrics: "service-metrics"
        alerts: "service-alerts"
    
    redis:
      host: "redis"
      port: 6379
      db: 0